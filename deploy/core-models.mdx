---
title: "Core Models"
---

[Roboflow Inference](https://inference.roboflow.com), which powers the Roboflow API, has several routes dedicated to foundation models for use in building vision workflows.

The core models currently supported through Roboflow Inference are:

* CLIP: CLIP understands images and text together, allowing it to associate them in a semantically meaningful way by being trained on a vast amount of internet text and images, [built by OpenAI](https://openai.com/research/clip). Available through the Roboflow API and on-device using Roboflow Inference. View an example application that uses the [on-device CLIP API](https://github.com/roboflow/inference/tree/main/examples/clip-search-engine).

* Segment Anything (SAM): Capable of locating arbitrary objects in images. It can also accept prompts to narrow in on specific objects, [built by Meta AI](https://segment-anything.com/). Available for on-device deployment using Roboflow Inference. View an [example application that uses the on-device SAM API](https://github.com/roboflow/inference/tree/main/examples/sam-client).

* Gaze: Identify the location someone is looking on a screen and the direction in which someone is looking. Available for on-device deployment using Roboflow Inference. [View an example project using the on-device gaze API](https://github.com/roboflow/inference/tree/main/examples/gaze-detection).